# 大模型学习路线

**LLM基座模型——LLM微调训练——LLM应用（如Agent和RAG）**

## LLM基座模型

这一部分主要是学习大模型的结构，建议各位有时间的话去看下GPT、ChatGLM、LLama、Mistral、Qwen这些热门模型的论文，然后比较下这些模型的异同，这些属于基本问题，很容易会问。

这一部分内容，各位必须要了解的

* Attention和Transformer的架构以及相关的一系列问题（不用多说了，把《Attention is All You Need》吃透，这个太基本了）
* 主流开源大模型的体系（分类）
* 大模型采用的各种Attention变体（Multi-query、Group-query、Slide window attention，以及加速优化的flash attention、flash attention v2）
* LayerNorm的各种改进（pre-norm、post-norm、deepnorm）
* 大模型训练环节（预训练、SFT、RLHF等等）
* MOE框架
* 位置编码的方式（绝对位置编码、相对位置编码、旋转编码等等）
* 常见评测数据集

学了可以加分的：

* 强化学习相关内容（RLHF、DPO等）
* 大模型压缩（量化、剪枝、蒸馏等）

## **LLM微调训练**

这一部分是核心，非常非常重要。现在大模型岗位基本上都是去做SFT的。

这一部分内容，各位必须要了解的：

* 常见PEFT方法（LoRA、Q-LoRA、P-tuning等等）
* 指令微调的数据格式
* 继续预训练
* 如何把英文大模型微调成中文大模型？
* 混合精度训练
* 如何评价训练后大模型的好坏，用什么指标？（根据场景来）

学了可以加分的：

* 大模型分布式训练及其框架（DeepSpeed等）

## LLM应用

这一部分根据个人情况，专攻一个方面即可（例如RAG或Agent选一个），主要看你项目做的啥，然后准备就行。

Agent部分我了解的不是很多，所以就不讲很细了。这里就讲下RAG要学些啥：

* RAG的流程
* RAG的文档识别方式（如何处理PDF和表格数据）
* RAG的文档划分方式
* RAG的召回策略以及一些改进方式（Hyde、RAG-fusion等）
* 如何评价RAG的好坏（召回结果怎么评价，LLM的回答结果怎么评价）
* 如何解决RAG的一些痛点问题，例如内容缺失、回答不全面等

另外还有些杂七杂八的知识也最好掌握：

* 如何解决大模型的重复生成问题？（高频考点）
* 大模型的推理优化（vLLM等）
* 如何对大模型进行知识注入
* 提示工程(例如CoT)

之后再说下如何准备项目以丰富自己的简历吧。我个人建议各位最好做一个LLM微调项目（必备）+一个LLM应用项目（Agent或RAG，推荐做Agent），这样的话能展示你技术栈的广度。网上有大量的相关项目，照着葫芦画瓢做就行了。

然后是刷Leetcode，这个不用多说，只要是搞计算机的都躲不开。Hot 100做完是必须的。

另外有的公司要求较高可能会在二面手撕代码，建议掌握attention、beamsearch、layernorm的代码写法。

然后，建议各位关注下知识星球 | 深度连接铁杆粉丝，运营高品质社群，知识变现的工具 (zsxq.com)。这个需要付费进入，里面有大量的LLM知识点总结和面经，我就是针对这个学的，很好用。

以及这个github整理的东西也挺不错的：liguodongiot/llm-action: 本项目旨在分享大模型相关技术原理以及实战经验。 (github.com)
