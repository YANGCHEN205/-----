{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pytorch复现Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "\n",
    "# 指定设备\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    \"实现位置编码\"\n",
    "    def __init__(self, d_model, dropout=0.1, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        # 初始化Shape为(max_len, d_model)的PE (positional encoding)\n",
    "        pe = torch.zeros(max_len, d_model).to(device)\n",
    "        \n",
    "        # 初始化一个tensor [[0, 1, 2, 3, ...]]\n",
    "        position = torch.arange(0, max_len).unsqueeze(1)\n",
    "        # 这里就是sin和cos括号中的内容，通过e和ln进行了变换\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2) * -(math.log(10000.0) / d_model))\n",
    "        \n",
    "        pe[:, 0::2] = torch.sin(position * div_term) # 计算PE(pos, 2i)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term) # 计算PE(pos, 2i+1)\n",
    "        \n",
    "        pe = pe.unsqueeze(0) # 为了方便计算，在最外面在unsqueeze出一个batch\n",
    "        \n",
    "        # 如果一个参数不参与梯度下降，但又希望保存model的时候将其保存下来\n",
    "        # 这个时候就可以用register_buffer\n",
    "        self.register_buffer(\"pe\", pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        x 为embedding后的inputs，例如(1,7, 128)，batch size为1,7个单词，单词维度为128\n",
    "        \"\"\"\n",
    "        # 将x和positional encoding相加。\n",
    "        x = x + self.pe[:, :x.size(1)].requires_grad_(False)\n",
    "        return self.dropout(x)\n",
    "\n",
    "\n",
    "embadding_layer = PositionalEncoding(d_model=20).to(device)\n",
    "input =  torch.rand(1,20,20).to(device)\n",
    "output = embadding_layer(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query shape: torch.Size([4, 10, 128])\n",
      "Key shape: torch.Size([4, 10, 128])\n",
      "Value shape: torch.Size([4, 10, 128])\n",
      "Output shape: torch.Size([4, 10, 128])\n",
      "Output: tensor([[[-6.0508e-02,  1.3901e-01,  1.9325e-01,  ...,  8.3878e-02,\n",
      "           2.0386e-01, -2.0819e-01],\n",
      "         [ 1.1281e-02,  1.2806e-01,  2.0734e-01,  ...,  7.7775e-02,\n",
      "           1.2669e-01, -2.7470e-01],\n",
      "         [-2.5256e-02,  1.1811e-01,  2.0855e-01,  ...,  1.2418e-01,\n",
      "           1.6183e-01, -2.1691e-01],\n",
      "         ...,\n",
      "         [-2.3038e-02,  1.3088e-01,  2.3977e-01,  ...,  9.1329e-02,\n",
      "           1.8616e-01, -2.5918e-01],\n",
      "         [ 2.7483e-02,  1.1962e-01,  2.2321e-01,  ...,  7.5202e-02,\n",
      "           1.8279e-01, -3.0807e-01],\n",
      "         [ 3.9515e-02,  1.0572e-01,  2.6153e-01,  ...,  6.6147e-02,\n",
      "           2.2817e-01, -2.3400e-01]],\n",
      "\n",
      "        [[-1.6332e-01,  2.2934e-01,  2.0568e-01,  ..., -4.3376e-03,\n",
      "           5.2436e-03,  5.7108e-02],\n",
      "         [-1.9431e-01,  3.5481e-01,  1.6259e-01,  ..., -6.6095e-04,\n",
      "           1.0266e-01,  6.0950e-02],\n",
      "         [-1.9254e-01,  2.2769e-01,  1.3201e-01,  ..., -7.4427e-03,\n",
      "           1.8944e-01,  3.5381e-02],\n",
      "         ...,\n",
      "         [-1.6734e-01,  1.5247e-01,  1.3610e-01,  ...,  7.5325e-06,\n",
      "           4.9325e-02,  5.2662e-02],\n",
      "         [-1.7043e-01,  2.5496e-01,  2.2546e-01,  ..., -6.6752e-03,\n",
      "           1.0879e-01,  8.3427e-02],\n",
      "         [-1.5741e-01,  2.3347e-01,  2.2763e-01,  ..., -8.5838e-02,\n",
      "           3.4672e-02,  6.0199e-02]],\n",
      "\n",
      "        [[-1.8534e-02,  7.1103e-02,  2.5751e-01,  ...,  1.5102e-01,\n",
      "           9.0854e-02,  3.0066e-02],\n",
      "         [ 3.9477e-02,  7.1121e-02,  3.4506e-01,  ...,  1.4238e-01,\n",
      "           1.4608e-01,  1.5933e-02],\n",
      "         [-5.8115e-02,  8.6300e-02,  3.1245e-01,  ...,  1.2312e-01,\n",
      "           9.3777e-02,  9.8685e-02],\n",
      "         ...,\n",
      "         [-8.8399e-02,  1.5634e-01,  3.0716e-01,  ...,  1.1557e-01,\n",
      "           1.2990e-01,  1.0701e-01],\n",
      "         [-3.1730e-03,  7.1654e-02,  3.3844e-01,  ...,  6.1323e-02,\n",
      "           1.1214e-01,  1.1019e-01],\n",
      "         [-2.1877e-02,  1.0416e-01,  3.5999e-01,  ...,  9.5803e-02,\n",
      "           1.2173e-01,  8.1425e-02]],\n",
      "\n",
      "        [[-1.6517e-01,  3.7174e-02,  2.7210e-01,  ...,  2.6621e-02,\n",
      "           1.0310e-01,  3.9369e-02],\n",
      "         [-1.1885e-01, -8.3696e-04,  2.1535e-01,  ...,  4.7118e-02,\n",
      "           5.6461e-02,  9.4080e-02],\n",
      "         [-1.0346e-01, -3.1942e-02,  2.6871e-01,  ...,  4.1572e-03,\n",
      "           3.0498e-02, -2.6709e-02],\n",
      "         ...,\n",
      "         [-1.6242e-01,  2.0136e-02,  2.1725e-01,  ...,  4.0687e-02,\n",
      "           7.6342e-02,  3.0044e-02],\n",
      "         [-1.5341e-01,  4.3055e-02,  2.2276e-01,  ...,  7.9135e-02,\n",
      "           9.0602e-02, -6.3121e-02],\n",
      "         [-1.2622e-01, -3.0980e-02,  2.3134e-01,  ...,  9.8899e-02,\n",
      "           4.7159e-02, -4.2937e-02]]], grad_fn=<ViewBackward0>)\n"
     ]
    }
   ],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    # n_heads：多头注意力的数量\n",
    "    # hid_dim：每个词输出的向量维度\n",
    "    def __init__(self, hid_dim, n_heads):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.hid_dim = hid_dim\n",
    "        self.n_heads = n_heads\n",
    "\n",
    "        # 强制 hid_dim 必须整除 h\n",
    "        assert hid_dim % n_heads == 0\n",
    "        # 定义 W_q 矩阵\n",
    "        self.w_q = nn.Linear(hid_dim, hid_dim)\n",
    "        # 定义 W_k 矩阵\n",
    "        self.w_k = nn.Linear(hid_dim, hid_dim)\n",
    "        # 定义 W_v 矩阵\n",
    "        self.w_v = nn.Linear(hid_dim, hid_dim)\n",
    "        self.fc  = nn.Linear(hid_dim, hid_dim)\n",
    "        # 缩放\n",
    "        self.scale = torch.sqrt(torch.FloatTensor([hid_dim // n_heads]))\n",
    "\n",
    "    def forward(self, query, key, value, mask=None):\n",
    "        # 注意 Q，K，V的在句子长度这一个维度的数值可以一样，可以不一样。\n",
    "        # K: [64,10,300], 假设batch_size 为 64，有 10 个词，每个词的 Query 向量是 300 维\n",
    "        # V: [64,10,300], 假设batch_size 为 64，有 10 个词，每个词的 Query 向量是 300 维\n",
    "        # Q: [64,12,300], 假设batch_size 为 64，有 12 个词，每个词的 Query 向量是 300 维\n",
    "        bsz = query.shape[0]\n",
    "        Q = self.w_q(query)\n",
    "        K = self.w_k(key)\n",
    "        V = self.w_v(value)\n",
    "        # 这里把 K Q V 矩阵拆分为多组注意力\n",
    "        # 最后一维就是是用 self.hid_dim // self.n_heads 来得到的，表示每组注意力的向量长度, 每个 head 的向量长度是：300/6=50\n",
    "        # 64 表示 batch size，6 表示有 6组注意力，10 表示有 10 词，50 表示每组注意力的词的向量长度\n",
    "        # K: [64,10,300] 拆分多组注意力 -> [64,10,6,50] 转置得到 -> [64,6,10,50]\n",
    "        # V: [64,10,300] 拆分多组注意力 -> [64,10,6,50] 转置得到 -> [64,6,10,50]\n",
    "        # Q: [64,12,300] 拆分多组注意力 -> [64,12,6,50] 转置得到 -> [64,6,12,50]\n",
    "        # 转置是为了把注意力的数量 6 放到前面，把 10 和 50 放到后面，方便下面计算\n",
    "        Q = Q.view(bsz, -1, self.n_heads, self.hid_dim //\n",
    "                   self.n_heads).permute(0, 2, 1, 3)\n",
    "        K = K.view(bsz, -1, self.n_heads, self.hid_dim //\n",
    "                   self.n_heads).permute(0, 2, 1, 3)\n",
    "        V = V.view(bsz, -1, self.n_heads, self.hid_dim //\n",
    "                   self.n_heads).permute(0, 2, 1, 3)\n",
    "\n",
    "        # 第 1 步：Q 乘以 K的转置，除以scale\n",
    "        # [64,6,12,50] * [64,6,50,10] = [64,6,12,10]\n",
    "        # attention：[64,6,12,10]\n",
    "        attention = torch.matmul(Q, K.permute(0, 1, 3, 2)) / self.scale\n",
    "\n",
    "        # 如果 mask 不为空，那么就把 mask 为 0 的位置的 attention 分数设置为 -1e10，这里用“0”来指示哪些位置的词向量不能被attention到，比如padding位置，当然也可以用“1”或者其他数字来指示，主要设计下面2行代码的改动。\n",
    "        if mask is not None:\n",
    "            attention = attention.masked_fill(mask == 0, -1e10)\n",
    "\n",
    "            # 第 2 步：计算上一步结果的 softmax，再经过 dropout，得到 attention。\n",
    "            # 注意，这里是对最后一维做 softmax，也就是在输入序列的维度做 softmax\n",
    "            # attention: [64,6,12,10]\n",
    "        attention = torch.softmax(attention, dim=-1)\n",
    "\n",
    "        # 第三步，attention结果与V相乘，得到多头注意力的结果\n",
    "        # [64,6,12,10] * [64,6,10,50] = [64,6,12,50]\n",
    "        # x: [64,6,12,50]\n",
    "        x = torch.matmul(attention, V)\n",
    "\n",
    "        # 因为 query 有 12 个词，所以把 12 放到前面，把 50 和 6 放到后面，方便下面拼接多组的结果\n",
    "        # x: [64,6,12,50] 转置-> [64,12,6,50]\n",
    "        x = x.permute(0, 2, 1, 3).contiguous()\n",
    "        # 这里的矩阵转换就是：把多组注意力的结果拼接起来\n",
    "        # 最终结果就是 [64,12,300]\n",
    "        # x: [64,12,6,50] -> [64,12,300]\n",
    "        x = x.view(bsz, -1, self.n_heads * (self.hid_dim // self.n_heads))\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "def test_multihead_attention():\n",
    "    d_model = 128  # 嵌入维度\n",
    "    num_heads = 8  # 头数\n",
    "    seq_len = 10  # 序列长度\n",
    "    batch_size = 4  # 批次大小\n",
    "\n",
    "    # 初始化MultiHeadAttention\n",
    "    mha = MultiHeadAttention(d_model, num_heads)\n",
    "\n",
    "    # 创建随机的输入张量 (batch_size, seq_len, d_model)\n",
    "    query = torch.randn(batch_size, seq_len, d_model)\n",
    "    key = torch.randn(batch_size, seq_len, d_model)\n",
    "    value = torch.randn(batch_size, seq_len, d_model)\n",
    "\n",
    "    # 获取Multi-Head Attention的输出\n",
    "    output = mha(query, key, value)\n",
    "\n",
    "    print(\"Query shape:\", query.shape)\n",
    "    print(\"Key shape:\", key.shape)\n",
    "    print(\"Value shape:\", value.shape)\n",
    "    print(\"Output shape:\", output.shape)\n",
    "    print(\"Output:\", output)\n",
    "\n",
    "# 执行测试样例\n",
    "test_multihead_attention()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 全部代码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformer(\n",
      "  (embedding): Embedding(10000, 512)\n",
      "  (positional_encoding): PositionalEncoding(\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (encoder_layers): ModuleList(\n",
      "    (0-5): 6 x EncoderLayer(\n",
      "      (self_attn): MultiHeadAttention(\n",
      "        (w_q): Linear(in_features=512, out_features=512, bias=True)\n",
      "        (w_k): Linear(in_features=512, out_features=512, bias=True)\n",
      "        (w_v): Linear(in_features=512, out_features=512, bias=True)\n",
      "        (fc): Linear(in_features=512, out_features=512, bias=True)\n",
      "      )\n",
      "      (feedforward): Feedforward(\n",
      "        (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "      )\n",
      "      (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (decoder_layers): ModuleList(\n",
      "    (0-5): 6 x DecoderLayer(\n",
      "      (self_attn): MultiHeadAttention(\n",
      "        (w_q): Linear(in_features=512, out_features=512, bias=True)\n",
      "        (w_k): Linear(in_features=512, out_features=512, bias=True)\n",
      "        (w_v): Linear(in_features=512, out_features=512, bias=True)\n",
      "        (fc): Linear(in_features=512, out_features=512, bias=True)\n",
      "      )\n",
      "      (enc_attn): MultiHeadAttention(\n",
      "        (w_q): Linear(in_features=512, out_features=512, bias=True)\n",
      "        (w_k): Linear(in_features=512, out_features=512, bias=True)\n",
      "        (w_v): Linear(in_features=512, out_features=512, bias=True)\n",
      "        (fc): Linear(in_features=512, out_features=512, bias=True)\n",
      "      )\n",
      "      (feedforward): Feedforward(\n",
      "        (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "      )\n",
      "      (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (fc_out): Linear(in_features=512, out_features=10000, bias=True)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "torch.Size([32, 20, 10000])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "\n",
    "# 指定设备\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    # n_heads：多头注意力的数量\n",
    "    # hid_dim：每个词输出的向量维度\n",
    "    def __init__(self, hid_dim, n_heads):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.hid_dim = hid_dim\n",
    "        self.n_heads = n_heads\n",
    "\n",
    "        # 强制 hid_dim 必须整除 h\n",
    "        assert hid_dim % n_heads == 0\n",
    "        # 定义 W_q 矩阵\n",
    "        self.w_q = nn.Linear(hid_dim, hid_dim)\n",
    "        # 定义 W_k 矩阵\n",
    "        self.w_k = nn.Linear(hid_dim, hid_dim)\n",
    "        # 定义 W_v 矩阵\n",
    "        self.w_v = nn.Linear(hid_dim, hid_dim)\n",
    "        self.fc  = nn.Linear(hid_dim, hid_dim)\n",
    "        # 缩放\n",
    "        self.scale = torch.sqrt(torch.FloatTensor([hid_dim // n_heads]))\n",
    "\n",
    "    def forward(self, query, key, value, mask=None):\n",
    "        # 注意 Q，K，V的在句子长度这一个维度的数值可以一样，可以不一样。\n",
    "        # K: [32,10,512], 假设batch_size 为 32，有 10 个词，每个词的 K 向量是 512 维\n",
    "        # V: [32,10,512], 假设batch_size 为 32，有 10 个词，每个词的 V 向量是 512 维\n",
    "        # Q: [32,10,512], 假设batch_size 为 32，有 10 个词，每个词的 Q 向量是 512 维\n",
    "        # k*x  v*x  w*x\n",
    "        bsz = query.shape[0]\n",
    "        Q = self.w_q(query)\n",
    "        K = self.w_k(key)\n",
    "        V = self.w_v(value)\n",
    "        # 这里把 K Q V 矩阵拆分为多组注意力\n",
    "        # 最后一维就是是用 self.hid_dim // self.n_heads 来得到的，表示每组注意力的向量长度, 每个 head 的向量长度是：512/8=64\n",
    "        # 32 表示 batch size，8 表示有 8组注意力，10 表示有 10 词，64 表示每组注意力的词的向量长度\n",
    "        # K: [32,10,512] 拆分多组注意力 -> [32,10,8,64] 转置得到 -> [32,8,10,64]\n",
    "        # V: [32,10,512] 拆分多组注意力 -> [32,10,8,64] 转置得到 -> [32,8,10,64]\n",
    "        # Q: [32,10,512] 拆分多组注意力 -> [32,10,8,64] 转置得到 -> [32,8,10,64]\n",
    "        # 转置是为了把注意力的数量 8 放到前面，把 10 和 64 放到后面，方便下面计算\n",
    "        Q = Q.view(bsz, -1, self.n_heads, self.hid_dim //\n",
    "                   self.n_heads).permute(0, 2, 1, 3)\n",
    "        K = K.view(bsz, -1, self.n_heads, self.hid_dim //\n",
    "                   self.n_heads).permute(0, 2, 1, 3)\n",
    "        V = V.view(bsz, -1, self.n_heads, self.hid_dim //\n",
    "                   self.n_heads).permute(0, 2, 1, 3)\n",
    "\n",
    "        # 第 1 步：Q 乘以 K的转置，除以scale\n",
    "        # [32,8,10,64] * [32,8,64,10] = [32,8,10,10]\n",
    "        # attention：[32,8,10,10]\n",
    "        attention = torch.matmul(Q, K.permute(0, 1, 3, 2)) / self.scale\n",
    "\n",
    "        # 如果 mask 不为空，那么就把 mask 为 0 的位置的 attention 分数设置为 -1e10，这里用“0”来指示哪些位置的词向量不能被attention到，比如padding位置，当然也可以用“1”或者其他数字来指示，主要设计下面2行代码的改动。\n",
    "        if mask is not None:\n",
    "            attention = attention.masked_fill(mask == 0, -1e10)\n",
    "\n",
    "            # 第 2 步：计算上一步结果的 softmax，再经过 dropout，得到 attention。\n",
    "            # 注意，这里是对最后一维做 softmax，也就是在输入序列的维度做 softmax\n",
    "            # attention: [32,8,10,10]\n",
    "        attention = torch.softmax(attention, dim=-1)   #softmax((Q*K)/scale)\n",
    "\n",
    "        # 第三步，attention结果与V相乘，得到多头注意力的结果\n",
    "        # [32,8,10,10] * [32,8,10,64] = [32,8,10,64]\n",
    "        # x: [32,8,10,64]\n",
    "        x = torch.matmul(attention, V)\n",
    "\n",
    "        # 因为 query 有 10 个词，所以把 10 放到前面，把 64 和 8 放到后面，方便下面拼接多组的结果\n",
    "        # x: [32,8,10,64] 转置-> [32,10,8,64]\n",
    "        x = x.permute(0, 2, 1, 3).contiguous()\n",
    "        # 这里的矩阵转换就是：把多组注意力的结果拼接起来\n",
    "        # 最终结果就是 [32,10,512]\n",
    "        # x: [32,10,8,64] -> [32,10,512]\n",
    "        x = x.view(bsz, -1, self.n_heads * (self.hid_dim // self.n_heads))\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "class Feedforward(nn.Module):\n",
    "    def __init__(self, d_model, d_ff, dropout=0.1):\n",
    "        super(Feedforward, self).__init__()\n",
    "        # 两层线性映射和激活函数ReLU\n",
    "        self.linear1 = nn.Linear(d_model, d_ff)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.linear2 = nn.Linear(d_ff, d_model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.nn.functional.relu(self.linear1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.linear2(x)\n",
    "        return x\n",
    "class PositionalEncoding(nn.Module):\n",
    "    \"实现位置编码\"\n",
    "    def __init__(self, d_model, dropout, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        # 初始化Shape为(max_len, d_model)的PE (positional encoding)\n",
    "        pe = torch.zeros(max_len, d_model).to(device)\n",
    "        \n",
    "        # 初始化一个tensor [[0, 1, 2, 3, ...]]\n",
    "        position = torch.arange(0, max_len).unsqueeze(1)\n",
    "        # 这里就是sin和cos括号中的内容，通过e和ln进行了变换\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2) * -(math.log(10000.0) / d_model))\n",
    "        \n",
    "        pe[:, 0::2] = torch.sin(position * div_term) # 计算PE(pos, 2i)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term) # 计算PE(pos, 2i+1)\n",
    "        \n",
    "        pe = pe.unsqueeze(0) # 为了方便计算，在最外面在unsqueeze出一个batch\n",
    "        \n",
    "        # 如果一个参数不参与梯度下降，但又希望保存model的时候将其保存下来\n",
    "        # 这个时候就可以用register_buffer\n",
    "        self.register_buffer(\"pe\", pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        x 为embedding后的inputs，例如(1,7, 128)，batch size为1,7个单词，单词维度为128\n",
    "        \"\"\"\n",
    "        # 将x和positional encoding相加。\n",
    "        x = x + self.pe[:, :x.size(1)].requires_grad_(False)\n",
    "        return self.dropout(x)\n",
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, n_heads, d_ff, dropout=0.1):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "        # 编码器层包含自注意力机制和前馈神经网络\n",
    "        self.self_attn   = MultiHeadAttention(d_model, n_heads)\n",
    "        self.feedforward = Feedforward(d_model, d_ff, dropout)\n",
    "        self.norm1   = nn.LayerNorm(d_model)\n",
    "        self.norm2   = nn.LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        # 自注意力机制\n",
    "        #输入[32,10,d_model]=[32,10,512],mask = [32,1,1,10]\n",
    "        attn_output = self.self_attn(x, x, x, mask)\n",
    "        x = x + self.dropout(attn_output)\n",
    "        x = self.norm1(x)\n",
    "        # x:[32,10,d_model]=[32,10,512]\n",
    "        # 前馈神经网络\n",
    "        ff_output = self.feedforward(x)\n",
    "        x = x + self.dropout(ff_output)\n",
    "        x = self.norm2(x)\n",
    "\n",
    "        return x\n",
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, n_heads, d_ff, dropout=0.1):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "        # 解码器层包含自注意力机制、编码器-解码器注意力机制和前馈神经网络\n",
    "        self.self_attn   = MultiHeadAttention(d_model, n_heads)\n",
    "        self.enc_attn    = MultiHeadAttention(d_model, n_heads)\n",
    "        self.feedforward = Feedforward(d_model, d_ff, dropout)\n",
    "        self.norm1   = nn.LayerNorm(d_model)\n",
    "        self.norm2   = nn.LayerNorm(d_model)\n",
    "        self.norm3   = nn.LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, enc_output, self_mask, context_mask):\n",
    "        # 自注意力机制\n",
    "        attn_output = self.self_attn(x, x, x, self_mask)\n",
    "        x           = x + self.dropout(attn_output)\n",
    "        x           = self.norm1(x)\n",
    "\n",
    "        # 编码器-解码器注意力机制\n",
    "        attn_output = self.enc_attn(x, enc_output, enc_output, context_mask)\n",
    "        x           = x + self.dropout(attn_output)\n",
    "        x           = self.norm2(x)\n",
    "\n",
    "        # 前馈神经网络\n",
    "        ff_output = self.feedforward(x)\n",
    "        x = x + self.dropout(ff_output)\n",
    "        x = self.norm3(x)\n",
    "\n",
    "        return x\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model, n_heads, n_encoder_layers, n_decoder_layers, d_ff, dropout=0.1):\n",
    "        super(Transformer, self).__init__()\n",
    "        # Transformer 模型包含词嵌入、位置编码、编码器和解码器\n",
    "        self.embedding           = nn.Embedding(vocab_size, d_model)\n",
    "        self.positional_encoding = PositionalEncoding(d_model,dropout)\n",
    "        self.encoder_layers      = nn.ModuleList([EncoderLayer(d_model, n_heads, d_ff, dropout) for _ in range(n_encoder_layers)])\n",
    "        self.decoder_layers      = nn.ModuleList([DecoderLayer(d_model, n_heads, d_ff, dropout) for _ in range(n_decoder_layers)])\n",
    "        self.fc_out              = nn.Linear(d_model, vocab_size)\n",
    "        self.dropout             = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, src, trg, src_mask, trg_mask):\n",
    "        # 词嵌入和位置编码\n",
    "        # src,traget 输入尺寸[32,10]\n",
    "        src = self.embedding(src)\n",
    "        src = self.positional_encoding(src)\n",
    "        trg = self.embedding(trg)\n",
    "        trg = self.positional_encoding(trg)\n",
    "        #输出之后[32,10,512]即[32,10,d_model]\n",
    "\n",
    "        # 编码器\n",
    "        for layer in self.encoder_layers:\n",
    "            src = layer(src, src_mask)\n",
    "\n",
    "        # 解码器\n",
    "        for layer in self.decoder_layers:\n",
    "            trg = layer(trg, src, trg_mask, src_mask)\n",
    "\n",
    "        # 输出层\n",
    "        output = self.fc_out(trg)\n",
    "\n",
    "        return output\n",
    "# 使用示例\n",
    "vocab_size = 10000  # 假设词汇表大小为10000\n",
    "d_model    = 512\n",
    "n_heads    = 8\n",
    "n_encoder_layers = 6\n",
    "n_decoder_layers = 6\n",
    "d_ff             = 2048\n",
    "dropout          = 0.1\n",
    "\n",
    "transformer_model = Transformer(vocab_size, d_model, n_heads, n_encoder_layers, n_decoder_layers, d_ff, dropout)\n",
    "\n",
    "# 定义输入，这里的输入是假设的，需要根据实际情况修改\n",
    "src = torch.randint(0, vocab_size, (32, 10))  # 源语言句子\n",
    "trg = torch.randint(0, vocab_size, (32, 20))  # 目标语言句子\n",
    "src_mask = (src != 0).unsqueeze(1).unsqueeze(2)  # 掩码，用于屏蔽填充的位置\n",
    "trg_mask = (trg != 0).unsqueeze(1).unsqueeze(2)  # 掩码，用于屏蔽填充的位置\n",
    "print(transformer_model)\n",
    "# 模型前向传播\n",
    "output = transformer_model(src, trg, src_mask, trg_mask)\n",
    "print(output.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# 步骤 1: 词汇表构建\n",
    "# 我们需要从训练数据中构建词汇表。这里我们可以使用一些常见的英文单词和中文词汇作为示例。\n",
    "# 假设的英文和中文词汇\n",
    "english_vocab = ['<pad>', '<unk>', '<sos>', '<eos>', 'hello', 'world', 'machine', 'learning']\n",
    "chinese_vocab = ['<pad>', '<unk>', '<sos>', '<eos>', '你好', '世界', '机器', '学习']\n",
    "\n",
    "# 创建词汇到索引的映射\n",
    "english_word2idx = {word: idx for idx, word in enumerate(english_vocab)}\n",
    "chinese_word2idx = {word: idx for idx, word in enumerate(chinese_vocab)}\n",
    "\n",
    "# 索引到词汇的映射，用于解码\n",
    "english_idx2word = {idx: word for word, idx in english_word2idx.items()}\n",
    "chinese_idx2word = {idx: word for word, idx in chinese_word2idx.items()}\n",
    "\n",
    "# 步骤 2: 数据预处理\n",
    "# 我们需要将文本数据转换为模型可以处理的索引序列。\n",
    "# 示例的训练数据对\n",
    "train_data = [\n",
    "    (\"hello world\", \"你好 世界\"),\n",
    "    (\"machine learning\", \"机器 学习\")\n",
    "]\n",
    "\n",
    "# 将文本转换为索引\n",
    "def text_to_index(sentences, word2idx):\n",
    "    indexed = []\n",
    "    for sentence in sentences:\n",
    "        sentence_idx = [word2idx.get(word, word2idx['<unk>']) for word in sentence.split()]\n",
    "        indexed.append(sentence_idx)\n",
    "    return indexed\n",
    "\n",
    "# 处理源语言和目标语言数据\n",
    "src_data = [pair[0] for pair in train_data]\n",
    "trg_data = [pair[1] for pair in train_data]\n",
    "\n",
    "src_indexed = text_to_index(src_data, english_word2idx)\n",
    "trg_indexed = text_to_index(trg_data, chinese_word2idx)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Administrator\\AppData\\Roaming\\Python\\Python39\\site-packages\\debugpy\\_vendored\\pydevd\\_pydevd_bundle\\pydevd_frame.py\", line 987, in trace_dispatch\n",
      "    self.do_wait_suspend(thread, frame, event, arg)\n",
      "  File \"C:\\Users\\Administrator\\AppData\\Roaming\\Python\\Python39\\site-packages\\debugpy\\_vendored\\pydevd\\_pydevd_bundle\\pydevd_frame.py\", line 164, in do_wait_suspend\n",
      "    self._args[0].do_wait_suspend(*args, **kwargs)\n",
      "  File \"C:\\Users\\Administrator\\AppData\\Roaming\\Python\\Python39\\site-packages\\debugpy\\_vendored\\pydevd\\pydevd.py\", line 2062, in do_wait_suspend\n",
      "    keep_suspended = self._do_wait_suspend(thread, frame, event, arg, suspend_type, from_this_thread, frames_tracker)\n",
      "  File \"C:\\Users\\Administrator\\AppData\\Roaming\\Python\\Python39\\site-packages\\debugpy\\_vendored\\pydevd\\pydevd.py\", line 2098, in _do_wait_suspend\n",
      "    time.sleep(0.01)\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [16], line 53\u001b[0m\n\u001b[0;32m     50\u001b[0m trg_y \u001b[38;5;241m=\u001b[39m trg_tensor[:, \u001b[38;5;241m1\u001b[39m:]\n\u001b[0;32m     52\u001b[0m \u001b[38;5;66;03m# 确保批量大小一致\u001b[39;00m\n\u001b[1;32m---> 53\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[43mF\u001b[49m\u001b[38;5;241m.\u001b[39mcross_entropy(output\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, output\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]), trg_y\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m), ignore_index\u001b[38;5;241m=\u001b[39mchinese_word2idx[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m<pad>\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "Cell \u001b[1;32mIn [16], line 53\u001b[0m\n\u001b[0;32m     50\u001b[0m trg_y \u001b[38;5;241m=\u001b[39m trg_tensor[:, \u001b[38;5;241m1\u001b[39m:]\n\u001b[0;32m     52\u001b[0m \u001b[38;5;66;03m# 确保批量大小一致\u001b[39;00m\n\u001b[1;32m---> 53\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[43mF\u001b[49m\u001b[38;5;241m.\u001b[39mcross_entropy(output\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, output\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]), trg_y\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m), ignore_index\u001b[38;5;241m=\u001b[39mchinese_word2idx[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m<pad>\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\debugpy\\_vendored\\pydevd\\_pydevd_bundle\\pydevd_frame.py:987\u001b[0m, in \u001b[0;36mPyDBFrame.trace_dispatch\u001b[1;34m(self, frame, event, arg)\u001b[0m\n\u001b[0;32m    985\u001b[0m \u001b[38;5;66;03m# if thread has a suspend flag, we suspend with a busy wait\u001b[39;00m\n\u001b[0;32m    986\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m info\u001b[38;5;241m.\u001b[39mpydev_state \u001b[38;5;241m==\u001b[39m STATE_SUSPEND:\n\u001b[1;32m--> 987\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdo_wait_suspend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mthread\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mframe\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mevent\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43marg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    988\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrace_dispatch\n\u001b[0;32m    989\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\debugpy\\_vendored\\pydevd\\_pydevd_bundle\\pydevd_frame.py:164\u001b[0m, in \u001b[0;36mPyDBFrame.do_wait_suspend\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    163\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdo_wait_suspend\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m--> 164\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_args[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mdo_wait_suspend(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\debugpy\\_vendored\\pydevd\\pydevd.py:2062\u001b[0m, in \u001b[0;36mPyDB.do_wait_suspend\u001b[1;34m(self, thread, frame, event, arg, exception_type)\u001b[0m\n\u001b[0;32m   2059\u001b[0m             from_this_thread\u001b[38;5;241m.\u001b[39mappend(frame_custom_thread_id)\n\u001b[0;32m   2061\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_threads_suspended_single_notification\u001b[38;5;241m.\u001b[39mnotify_thread_suspended(thread_id, stop_reason):\n\u001b[1;32m-> 2062\u001b[0m         keep_suspended \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_do_wait_suspend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mthread\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mframe\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mevent\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43marg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msuspend_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrom_this_thread\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mframes_tracker\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2064\u001b[0m frames_list \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   2066\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m keep_suspended:\n\u001b[0;32m   2067\u001b[0m     \u001b[38;5;66;03m# This means that we should pause again after a set next statement.\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\debugpy\\_vendored\\pydevd\\pydevd.py:2098\u001b[0m, in \u001b[0;36mPyDB._do_wait_suspend\u001b[1;34m(self, thread, frame, event, arg, suspend_type, from_this_thread, frames_tracker)\u001b[0m\n\u001b[0;32m   2095\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_input_hook()\n\u001b[0;32m   2097\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprocess_internal_commands()\n\u001b[1;32m-> 2098\u001b[0m     \u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0.01\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2100\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcancel_async_evaluation(get_current_thread_id(thread), \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mid\u001b[39m(frame)))\n\u001b[0;32m   2102\u001b[0m \u001b[38;5;66;03m# process any stepping instructions\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# 初始化模型\n",
    "transformer_model = Transformer(\n",
    "    vocab_size=len(english_vocab), \n",
    "    d_model=512, \n",
    "    n_heads=8, \n",
    "    n_encoder_layers=6, \n",
    "    n_decoder_layers=6, \n",
    "    d_ff=2048, \n",
    "    dropout=0.1\n",
    ")\n",
    "\n",
    "# 定义优化器\n",
    "optimizer = optim.Adam(transformer_model.parameters(), lr=0.001)\n",
    "\n",
    "# 示例的训练数据对\n",
    "train_data = [\n",
    "    (\"hello world\", \"你好 世界\"),\n",
    "    (\"machine learning\", \"机器 学习\")\n",
    "]\n",
    "\n",
    "# 将文本转换为索引的函数中，确保批量大小一致\n",
    "def text_to_index(sentences, word2idx):\n",
    "    indexed = []\n",
    "    max_len = max(len(s.split()) for s in sentences)  # 找到最长句子的长度\n",
    "    for sentence in sentences:\n",
    "        sentence_idx = [word2idx.get(word, word2idx['<unk>']) for word in sentence.split()]\n",
    "        sentence_idx += [word2idx['<pad>']] * (max_len - len(sentence_idx))  # 填充到最长句子长度\n",
    "        indexed.append(sentence_idx)\n",
    "    return indexed\n",
    "\n",
    "# 处理源语言和目标语言数据\n",
    "src_indexed = text_to_index([pair[0] for pair in train_data], english_word2idx)\n",
    "trg_indexed = text_to_index([pair[1] for pair in train_data], chinese_word2idx)\n",
    "\n",
    "# 训练模型\n",
    "for src, trg in zip(src_indexed, trg_indexed):\n",
    "    src_tensor = torch.tensor([src])  # 增加 batch 维度\n",
    "    trg_tensor = torch.tensor([trg])  # 增加 batch 维度\n",
    "\n",
    "    # 生成掩码\n",
    "    src_mask = (src_tensor != english_word2idx['<pad>']).unsqueeze(1).unsqueeze(2)\n",
    "    trg_mask = (trg_tensor != chinese_word2idx['<pad>']).unsqueeze(1).unsqueeze(2)\n",
    "\n",
    "    # 模型前向传播\n",
    "    output = transformer_model(src_tensor, trg_tensor, src_mask, trg_mask)\n",
    "    trg_y = trg_tensor[:, 1:]\n",
    "\n",
    "    # 确保批量大小一致\n",
    "    loss = F.cross_entropy(output.view(-1, output.shape[-1]), trg_y.view(-1), ignore_index=chinese_word2idx['<pad>'])\n",
    "    # 其余的训练代码...\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
