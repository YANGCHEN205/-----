{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pytorch复现Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "\n",
    "# 指定设备\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    \"实现位置编码\"\n",
    "    def __init__(self, d_model, dropout=0.1, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        # 初始化Shape为(max_len, d_model)的PE (positional encoding)\n",
    "        pe = torch.zeros(max_len, d_model).to(device)\n",
    "        \n",
    "        # 初始化一个tensor [[0, 1, 2, 3, ...]]\n",
    "        position = torch.arange(0, max_len).unsqueeze(1)\n",
    "        # 这里就是sin和cos括号中的内容，通过e和ln进行了变换\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2) * -(math.log(10000.0) / d_model))\n",
    "        \n",
    "        pe[:, 0::2] = torch.sin(position * div_term) # 计算PE(pos, 2i)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term) # 计算PE(pos, 2i+1)\n",
    "        \n",
    "        pe = pe.unsqueeze(0) # 为了方便计算，在最外面在unsqueeze出一个batch\n",
    "        \n",
    "        # 如果一个参数不参与梯度下降，但又希望保存model的时候将其保存下来\n",
    "        # 这个时候就可以用register_buffer\n",
    "        self.register_buffer(\"pe\", pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        x 为embedding后的inputs，例如(1,7, 128)，batch size为1,7个单词，单词维度为128\n",
    "        \"\"\"\n",
    "        # 将x和positional encoding相加。\n",
    "        x = x + self.pe[:, :x.size(1)].requires_grad_(False)\n",
    "        return self.dropout(x)\n",
    "\n",
    "\n",
    "embadding_layer = PositionalEncoding(d_model=20).to(device)\n",
    "input =  torch.rand(1,20,20).to(device)\n",
    "output = embadding_layer(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query shape: torch.Size([4, 10, 128])\n",
      "Key shape: torch.Size([4, 10, 128])\n",
      "Value shape: torch.Size([4, 10, 128])\n",
      "Output shape: torch.Size([4, 10, 128])\n",
      "Output: tensor([[[-6.0508e-02,  1.3901e-01,  1.9325e-01,  ...,  8.3878e-02,\n",
      "           2.0386e-01, -2.0819e-01],\n",
      "         [ 1.1281e-02,  1.2806e-01,  2.0734e-01,  ...,  7.7775e-02,\n",
      "           1.2669e-01, -2.7470e-01],\n",
      "         [-2.5256e-02,  1.1811e-01,  2.0855e-01,  ...,  1.2418e-01,\n",
      "           1.6183e-01, -2.1691e-01],\n",
      "         ...,\n",
      "         [-2.3038e-02,  1.3088e-01,  2.3977e-01,  ...,  9.1329e-02,\n",
      "           1.8616e-01, -2.5918e-01],\n",
      "         [ 2.7483e-02,  1.1962e-01,  2.2321e-01,  ...,  7.5202e-02,\n",
      "           1.8279e-01, -3.0807e-01],\n",
      "         [ 3.9515e-02,  1.0572e-01,  2.6153e-01,  ...,  6.6147e-02,\n",
      "           2.2817e-01, -2.3400e-01]],\n",
      "\n",
      "        [[-1.6332e-01,  2.2934e-01,  2.0568e-01,  ..., -4.3376e-03,\n",
      "           5.2436e-03,  5.7108e-02],\n",
      "         [-1.9431e-01,  3.5481e-01,  1.6259e-01,  ..., -6.6095e-04,\n",
      "           1.0266e-01,  6.0950e-02],\n",
      "         [-1.9254e-01,  2.2769e-01,  1.3201e-01,  ..., -7.4427e-03,\n",
      "           1.8944e-01,  3.5381e-02],\n",
      "         ...,\n",
      "         [-1.6734e-01,  1.5247e-01,  1.3610e-01,  ...,  7.5325e-06,\n",
      "           4.9325e-02,  5.2662e-02],\n",
      "         [-1.7043e-01,  2.5496e-01,  2.2546e-01,  ..., -6.6752e-03,\n",
      "           1.0879e-01,  8.3427e-02],\n",
      "         [-1.5741e-01,  2.3347e-01,  2.2763e-01,  ..., -8.5838e-02,\n",
      "           3.4672e-02,  6.0199e-02]],\n",
      "\n",
      "        [[-1.8534e-02,  7.1103e-02,  2.5751e-01,  ...,  1.5102e-01,\n",
      "           9.0854e-02,  3.0066e-02],\n",
      "         [ 3.9477e-02,  7.1121e-02,  3.4506e-01,  ...,  1.4238e-01,\n",
      "           1.4608e-01,  1.5933e-02],\n",
      "         [-5.8115e-02,  8.6300e-02,  3.1245e-01,  ...,  1.2312e-01,\n",
      "           9.3777e-02,  9.8685e-02],\n",
      "         ...,\n",
      "         [-8.8399e-02,  1.5634e-01,  3.0716e-01,  ...,  1.1557e-01,\n",
      "           1.2990e-01,  1.0701e-01],\n",
      "         [-3.1730e-03,  7.1654e-02,  3.3844e-01,  ...,  6.1323e-02,\n",
      "           1.1214e-01,  1.1019e-01],\n",
      "         [-2.1877e-02,  1.0416e-01,  3.5999e-01,  ...,  9.5803e-02,\n",
      "           1.2173e-01,  8.1425e-02]],\n",
      "\n",
      "        [[-1.6517e-01,  3.7174e-02,  2.7210e-01,  ...,  2.6621e-02,\n",
      "           1.0310e-01,  3.9369e-02],\n",
      "         [-1.1885e-01, -8.3696e-04,  2.1535e-01,  ...,  4.7118e-02,\n",
      "           5.6461e-02,  9.4080e-02],\n",
      "         [-1.0346e-01, -3.1942e-02,  2.6871e-01,  ...,  4.1572e-03,\n",
      "           3.0498e-02, -2.6709e-02],\n",
      "         ...,\n",
      "         [-1.6242e-01,  2.0136e-02,  2.1725e-01,  ...,  4.0687e-02,\n",
      "           7.6342e-02,  3.0044e-02],\n",
      "         [-1.5341e-01,  4.3055e-02,  2.2276e-01,  ...,  7.9135e-02,\n",
      "           9.0602e-02, -6.3121e-02],\n",
      "         [-1.2622e-01, -3.0980e-02,  2.3134e-01,  ...,  9.8899e-02,\n",
      "           4.7159e-02, -4.2937e-02]]], grad_fn=<ViewBackward0>)\n"
     ]
    }
   ],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    # n_heads：多头注意力的数量\n",
    "    # hid_dim：每个词输出的向量维度\n",
    "    def __init__(self, hid_dim, n_heads):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.hid_dim = hid_dim\n",
    "        self.n_heads = n_heads\n",
    "\n",
    "        # 强制 hid_dim 必须整除 h\n",
    "        assert hid_dim % n_heads == 0\n",
    "        # 定义 W_q 矩阵\n",
    "        self.w_q = nn.Linear(hid_dim, hid_dim)\n",
    "        # 定义 W_k 矩阵\n",
    "        self.w_k = nn.Linear(hid_dim, hid_dim)\n",
    "        # 定义 W_v 矩阵\n",
    "        self.w_v = nn.Linear(hid_dim, hid_dim)\n",
    "        self.fc  = nn.Linear(hid_dim, hid_dim)\n",
    "        # 缩放\n",
    "        self.scale = torch.sqrt(torch.FloatTensor([hid_dim // n_heads]))\n",
    "\n",
    "    def forward(self, query, key, value, mask=None):\n",
    "        # 注意 Q，K，V的在句子长度这一个维度的数值可以一样，可以不一样。\n",
    "        # K: [64,10,300], 假设batch_size 为 64，有 10 个词，每个词的 Query 向量是 300 维\n",
    "        # V: [64,10,300], 假设batch_size 为 64，有 10 个词，每个词的 Query 向量是 300 维\n",
    "        # Q: [64,12,300], 假设batch_size 为 64，有 12 个词，每个词的 Query 向量是 300 维\n",
    "        bsz = query.shape[0]\n",
    "        Q = self.w_q(query)\n",
    "        K = self.w_k(key)\n",
    "        V = self.w_v(value)\n",
    "        # 这里把 K Q V 矩阵拆分为多组注意力\n",
    "        # 最后一维就是是用 self.hid_dim // self.n_heads 来得到的，表示每组注意力的向量长度, 每个 head 的向量长度是：300/6=50\n",
    "        # 64 表示 batch size，6 表示有 6组注意力，10 表示有 10 词，50 表示每组注意力的词的向量长度\n",
    "        # K: [64,10,300] 拆分多组注意力 -> [64,10,6,50] 转置得到 -> [64,6,10,50]\n",
    "        # V: [64,10,300] 拆分多组注意力 -> [64,10,6,50] 转置得到 -> [64,6,10,50]\n",
    "        # Q: [64,12,300] 拆分多组注意力 -> [64,12,6,50] 转置得到 -> [64,6,12,50]\n",
    "        # 转置是为了把注意力的数量 6 放到前面，把 10 和 50 放到后面，方便下面计算\n",
    "        Q = Q.view(bsz, -1, self.n_heads, self.hid_dim //\n",
    "                   self.n_heads).permute(0, 2, 1, 3)\n",
    "        K = K.view(bsz, -1, self.n_heads, self.hid_dim //\n",
    "                   self.n_heads).permute(0, 2, 1, 3)\n",
    "        V = V.view(bsz, -1, self.n_heads, self.hid_dim //\n",
    "                   self.n_heads).permute(0, 2, 1, 3)\n",
    "\n",
    "        # 第 1 步：Q 乘以 K的转置，除以scale\n",
    "        # [64,6,12,50] * [64,6,50,10] = [64,6,12,10]\n",
    "        # attention：[64,6,12,10]\n",
    "        attention = torch.matmul(Q, K.permute(0, 1, 3, 2)) / self.scale\n",
    "\n",
    "        # 如果 mask 不为空，那么就把 mask 为 0 的位置的 attention 分数设置为 -1e10，这里用“0”来指示哪些位置的词向量不能被attention到，比如padding位置，当然也可以用“1”或者其他数字来指示，主要设计下面2行代码的改动。\n",
    "        if mask is not None:\n",
    "            attention = attention.masked_fill(mask == 0, -1e10)\n",
    "\n",
    "            # 第 2 步：计算上一步结果的 softmax，再经过 dropout，得到 attention。\n",
    "            # 注意，这里是对最后一维做 softmax，也就是在输入序列的维度做 softmax\n",
    "            # attention: [64,6,12,10]\n",
    "        attention = torch.softmax(attention, dim=-1)\n",
    "\n",
    "        # 第三步，attention结果与V相乘，得到多头注意力的结果\n",
    "        # [64,6,12,10] * [64,6,10,50] = [64,6,12,50]\n",
    "        # x: [64,6,12,50]\n",
    "        x = torch.matmul(attention, V)\n",
    "\n",
    "        # 因为 query 有 12 个词，所以把 12 放到前面，把 50 和 6 放到后面，方便下面拼接多组的结果\n",
    "        # x: [64,6,12,50] 转置-> [64,12,6,50]\n",
    "        x = x.permute(0, 2, 1, 3).contiguous()\n",
    "        # 这里的矩阵转换就是：把多组注意力的结果拼接起来\n",
    "        # 最终结果就是 [64,12,300]\n",
    "        # x: [64,12,6,50] -> [64,12,300]\n",
    "        x = x.view(bsz, -1, self.n_heads * (self.hid_dim // self.n_heads))\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "def test_multihead_attention():\n",
    "    d_model = 128  # 嵌入维度\n",
    "    num_heads = 8  # 头数\n",
    "    seq_len = 10  # 序列长度\n",
    "    batch_size = 4  # 批次大小\n",
    "\n",
    "    # 初始化MultiHeadAttention\n",
    "    mha = MultiHeadAttention(d_model, num_heads)\n",
    "\n",
    "    # 创建随机的输入张量 (batch_size, seq_len, d_model)\n",
    "    query = torch.randn(batch_size, seq_len, d_model)\n",
    "    key = torch.randn(batch_size, seq_len, d_model)\n",
    "    value = torch.randn(batch_size, seq_len, d_model)\n",
    "\n",
    "    # 获取Multi-Head Attention的输出\n",
    "    output = mha(query, key, value)\n",
    "\n",
    "    print(\"Query shape:\", query.shape)\n",
    "    print(\"Key shape:\", key.shape)\n",
    "    print(\"Value shape:\", value.shape)\n",
    "    print(\"Output shape:\", output.shape)\n",
    "    print(\"Output:\", output)\n",
    "\n",
    "# 执行测试样例\n",
    "test_multihead_attention()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 全部代码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformer(\n",
      "  (embedding): Embedding(10000, 512)\n",
      "  (positional_encoding): PositionalEncoding(\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (encoder_layers): ModuleList(\n",
      "    (0-5): 6 x EncoderLayer(\n",
      "      (self_attn): MultiHeadAttention(\n",
      "        (w_q): Linear(in_features=512, out_features=512, bias=True)\n",
      "        (w_k): Linear(in_features=512, out_features=512, bias=True)\n",
      "        (w_v): Linear(in_features=512, out_features=512, bias=True)\n",
      "        (fc): Linear(in_features=512, out_features=512, bias=True)\n",
      "      )\n",
      "      (feedforward): Feedforward(\n",
      "        (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "      )\n",
      "      (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (decoder_layers): ModuleList(\n",
      "    (0-5): 6 x DecoderLayer(\n",
      "      (self_attn): MultiHeadAttention(\n",
      "        (w_q): Linear(in_features=512, out_features=512, bias=True)\n",
      "        (w_k): Linear(in_features=512, out_features=512, bias=True)\n",
      "        (w_v): Linear(in_features=512, out_features=512, bias=True)\n",
      "        (fc): Linear(in_features=512, out_features=512, bias=True)\n",
      "      )\n",
      "      (enc_attn): MultiHeadAttention(\n",
      "        (w_q): Linear(in_features=512, out_features=512, bias=True)\n",
      "        (w_k): Linear(in_features=512, out_features=512, bias=True)\n",
      "        (w_v): Linear(in_features=512, out_features=512, bias=True)\n",
      "        (fc): Linear(in_features=512, out_features=512, bias=True)\n",
      "      )\n",
      "      (feedforward): Feedforward(\n",
      "        (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "      )\n",
      "      (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (fc_out): Linear(in_features=512, out_features=10000, bias=True)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "torch.Size([32, 20, 10000])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "\n",
    "# 指定设备\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    # n_heads：多头注意力的数量\n",
    "    # hid_dim：每个词输出的向量维度\n",
    "    def __init__(self, hid_dim, n_heads):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.hid_dim = hid_dim\n",
    "        self.n_heads = n_heads\n",
    "\n",
    "        # 强制 hid_dim 必须整除 h\n",
    "        assert hid_dim % n_heads == 0\n",
    "        # 定义 W_q 矩阵\n",
    "        self.w_q = nn.Linear(hid_dim, hid_dim)\n",
    "        # 定义 W_k 矩阵\n",
    "        self.w_k = nn.Linear(hid_dim, hid_dim)\n",
    "        # 定义 W_v 矩阵\n",
    "        self.w_v = nn.Linear(hid_dim, hid_dim)\n",
    "        self.fc  = nn.Linear(hid_dim, hid_dim)\n",
    "        # 缩放\n",
    "        self.scale = torch.sqrt(torch.FloatTensor([hid_dim // n_heads]))\n",
    "\n",
    "    def forward(self, query, key, value, mask=None):\n",
    "        # 注意 Q，K，V的在句子长度这一个维度的数值可以一样，可以不一样。\n",
    "        # K: [32,10,512], 假设batch_size 为 32，有 10 个词，每个词的 K 向量是 512 维\n",
    "        # V: [32,10,512], 假设batch_size 为 32，有 10 个词，每个词的 V 向量是 512 维\n",
    "        # Q: [32,10,512], 假设batch_size 为 32，有 10 个词，每个词的 Q 向量是 512 维\n",
    "        # k*x  v*x  w*x\n",
    "        bsz = query.shape[0]\n",
    "        Q = self.w_q(query)\n",
    "        K = self.w_k(key)\n",
    "        V = self.w_v(value)\n",
    "        # 这里把 K Q V 矩阵拆分为多组注意力\n",
    "        # 最后一维就是是用 self.hid_dim // self.n_heads 来得到的，表示每组注意力的向量长度, 每个 head 的向量长度是：512/8=64\n",
    "        # 32 表示 batch size，8 表示有 8组注意力，10 表示有 10 词，64 表示每组注意力的词的向量长度\n",
    "        # K: [32,10,512] 拆分多组注意力 -> [32,10,8,64] 转置得到 -> [32,8,10,64]\n",
    "        # V: [32,10,512] 拆分多组注意力 -> [32,10,8,64] 转置得到 -> [32,8,10,64]\n",
    "        # Q: [32,10,512] 拆分多组注意力 -> [32,10,8,64] 转置得到 -> [32,8,10,64]\n",
    "        # 转置是为了把注意力的数量 8 放到前面，把 10 和 64 放到后面，方便下面计算\n",
    "        Q = Q.view(bsz, -1, self.n_heads, self.hid_dim //\n",
    "                   self.n_heads).permute(0, 2, 1, 3)\n",
    "        K = K.view(bsz, -1, self.n_heads, self.hid_dim //\n",
    "                   self.n_heads).permute(0, 2, 1, 3)\n",
    "        V = V.view(bsz, -1, self.n_heads, self.hid_dim //\n",
    "                   self.n_heads).permute(0, 2, 1, 3)\n",
    "\n",
    "        # 第 1 步：Q 乘以 K的转置，除以scale\n",
    "        # [32,8,10,64] * [32,8,64,10] = [32,8,10,10]\n",
    "        # attention：[32,8,10,10]\n",
    "        attention = torch.matmul(Q, K.permute(0, 1, 3, 2)) / self.scale\n",
    "\n",
    "        # 如果 mask 不为空，那么就把 mask 为 0 的位置的 attention 分数设置为 -1e10，这里用“0”来指示哪些位置的词向量不能被attention到，比如padding位置，当然也可以用“1”或者其他数字来指示，主要设计下面2行代码的改动。\n",
    "        if mask is not None:\n",
    "            attention = attention.masked_fill(mask == 0, -1e10)\n",
    "\n",
    "            # 第 2 步：计算上一步结果的 softmax，再经过 dropout，得到 attention。\n",
    "            # 注意，这里是对最后一维做 softmax，也就是在输入序列的维度做 softmax\n",
    "            # attention: [32,8,10,10]\n",
    "        attention = torch.softmax(attention, dim=-1)   #softmax((Q*K)/scale)\n",
    "\n",
    "        # 第三步，attention结果与V相乘，得到多头注意力的结果\n",
    "        # [32,8,10,10] * [32,8,10,64] = [32,8,10,64]\n",
    "        # x: [32,8,10,64]\n",
    "        x = torch.matmul(attention, V)\n",
    "\n",
    "        # 因为 query 有 10 个词，所以把 10 放到前面，把 64 和 8 放到后面，方便下面拼接多组的结果\n",
    "        # x: [32,8,10,64] 转置-> [32,10,8,64]\n",
    "        x = x.permute(0, 2, 1, 3).contiguous()\n",
    "        # 这里的矩阵转换就是：把多组注意力的结果拼接起来\n",
    "        # 最终结果就是 [32,10,512]\n",
    "        # x: [32,10,8,64] -> [32,10,512]\n",
    "        x = x.view(bsz, -1, self.n_heads * (self.hid_dim // self.n_heads))\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "class Feedforward(nn.Module):\n",
    "    def __init__(self, d_model, d_ff, dropout=0.1):\n",
    "        super(Feedforward, self).__init__()\n",
    "        # 两层线性映射和激活函数ReLU\n",
    "        self.linear1 = nn.Linear(d_model, d_ff)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.linear2 = nn.Linear(d_ff, d_model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.nn.functional.relu(self.linear1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.linear2(x)\n",
    "        return x\n",
    "class PositionalEncoding(nn.Module):\n",
    "    \"实现位置编码\"\n",
    "    def __init__(self, d_model, dropout, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        # 初始化Shape为(max_len, d_model)的PE (positional encoding)\n",
    "        pe = torch.zeros(max_len, d_model).to(device)\n",
    "        \n",
    "        # 初始化一个tensor [[0, 1, 2, 3, ...]]\n",
    "        position = torch.arange(0, max_len).unsqueeze(1)\n",
    "        # 这里就是sin和cos括号中的内容，通过e和ln进行了变换\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2) * -(math.log(10000.0) / d_model))\n",
    "        \n",
    "        pe[:, 0::2] = torch.sin(position * div_term) # 计算PE(pos, 2i)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term) # 计算PE(pos, 2i+1)\n",
    "        \n",
    "        pe = pe.unsqueeze(0) # 为了方便计算，在最外面在unsqueeze出一个batch\n",
    "        \n",
    "        # 如果一个参数不参与梯度下降，但又希望保存model的时候将其保存下来\n",
    "        # 这个时候就可以用register_buffer\n",
    "        self.register_buffer(\"pe\", pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        x 为embedding后的inputs，例如(1,7, 128)，batch size为1,7个单词，单词维度为128\n",
    "        \"\"\"\n",
    "        # 将x和positional encoding相加。\n",
    "        x = x + self.pe[:, :x.size(1)].requires_grad_(False)\n",
    "        return self.dropout(x)\n",
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, n_heads, d_ff, dropout=0.1):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "        # 编码器层包含自注意力机制和前馈神经网络\n",
    "        self.self_attn   = MultiHeadAttention(d_model, n_heads)\n",
    "        self.feedforward = Feedforward(d_model, d_ff, dropout)\n",
    "        self.norm1   = nn.LayerNorm(d_model)\n",
    "        self.norm2   = nn.LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        # 自注意力机制\n",
    "        #输入[32,10,d_model]=[32,10,512],mask = [32,1,1,10]\n",
    "        attn_output = self.self_attn(x, x, x, mask)\n",
    "        x = x + self.dropout(attn_output)\n",
    "        x = self.norm1(x)\n",
    "        # x:[32,10,d_model]=[32,10,512]\n",
    "        # 前馈神经网络\n",
    "        ff_output = self.feedforward(x)\n",
    "        x = x + self.dropout(ff_output)\n",
    "        x = self.norm2(x)\n",
    "\n",
    "        return x\n",
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, n_heads, d_ff, dropout=0.1):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "        # 解码器层包含自注意力机制、编码器-解码器注意力机制和前馈神经网络\n",
    "        self.self_attn   = MultiHeadAttention(d_model, n_heads)\n",
    "        self.enc_attn    = MultiHeadAttention(d_model, n_heads)\n",
    "        self.feedforward = Feedforward(d_model, d_ff, dropout)\n",
    "        self.norm1   = nn.LayerNorm(d_model)\n",
    "        self.norm2   = nn.LayerNorm(d_model)\n",
    "        self.norm3   = nn.LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, enc_output, self_mask, context_mask):\n",
    "        # 自注意力机制\n",
    "        attn_output = self.self_attn(x, x, x, self_mask)\n",
    "        x           = x + self.dropout(attn_output)\n",
    "        x           = self.norm1(x)\n",
    "\n",
    "        # 编码器-解码器注意力机制\n",
    "        attn_output = self.enc_attn(x, enc_output, enc_output, context_mask)\n",
    "        x           = x + self.dropout(attn_output)\n",
    "        x           = self.norm2(x)\n",
    "\n",
    "        # 前馈神经网络\n",
    "        ff_output = self.feedforward(x)\n",
    "        x = x + self.dropout(ff_output)\n",
    "        x = self.norm3(x)\n",
    "\n",
    "        return x\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model, n_heads, n_encoder_layers, n_decoder_layers, d_ff, dropout=0.1):\n",
    "        super(Transformer, self).__init__()\n",
    "        # Transformer 模型包含词嵌入、位置编码、编码器和解码器\n",
    "        self.embedding           = nn.Embedding(vocab_size, d_model)\n",
    "        self.positional_encoding = PositionalEncoding(d_model,dropout)\n",
    "        self.encoder_layers      = nn.ModuleList([EncoderLayer(d_model, n_heads, d_ff, dropout) for _ in range(n_encoder_layers)])\n",
    "        self.decoder_layers      = nn.ModuleList([DecoderLayer(d_model, n_heads, d_ff, dropout) for _ in range(n_decoder_layers)])\n",
    "        self.fc_out              = nn.Linear(d_model, vocab_size)\n",
    "        self.dropout             = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, src, trg, src_mask, trg_mask):\n",
    "        # 词嵌入和位置编码\n",
    "        # src,traget 输入尺寸[32,10]\n",
    "        src = self.embedding(src)\n",
    "        src = self.positional_encoding(src)\n",
    "        trg = self.embedding(trg)\n",
    "        trg = self.positional_encoding(trg)\n",
    "        #输出之后[32,10,512]即[32,10,d_model]\n",
    "\n",
    "        # 编码器\n",
    "        for layer in self.encoder_layers:\n",
    "            src = layer(src, src_mask)\n",
    "\n",
    "        # 解码器\n",
    "        for layer in self.decoder_layers:\n",
    "            trg = layer(trg, src, trg_mask, src_mask)\n",
    "\n",
    "        # 输出层\n",
    "        output = self.fc_out(trg)\n",
    "\n",
    "        return output\n",
    "# 使用示例\n",
    "vocab_size = 10000  # 假设词汇表大小为10000\n",
    "d_model    = 512\n",
    "n_heads    = 8\n",
    "n_encoder_layers = 6\n",
    "n_decoder_layers = 6\n",
    "d_ff             = 2048\n",
    "dropout          = 0.1\n",
    "\n",
    "transformer_model = Transformer(vocab_size, d_model, n_heads, n_encoder_layers, n_decoder_layers, d_ff, dropout)\n",
    "\n",
    "# 定义输入，这里的输入是假设的，需要根据实际情况修改\n",
    "src = torch.randint(0, vocab_size, (32, 10))  # 源语言句子\n",
    "trg = torch.randint(0, vocab_size, (32, 20))  # 目标语言句子\n",
    "src_mask = (src != 0).unsqueeze(1).unsqueeze(2)  # 掩码，用于屏蔽填充的位置\n",
    "trg_mask = (trg != 0).unsqueeze(1).unsqueeze(2)  # 掩码，用于屏蔽填充的位置\n",
    "print(transformer_model)\n",
    "# 模型前向传播\n",
    "output = transformer_model(src, trg, src_mask, trg_mask)\n",
    "print(output.shape)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
