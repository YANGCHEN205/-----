# 微调流程

一般来说, 垂直领域的现状就是大家积累很多电子数据,从现实出发,第一步可以先做增量训练.所以会把模型分成3个阶段:


## 第一阶段:(Continue PreTraining)

增量预训练，在海量领域文档数据上二次预训练GPT模型，以注入领域知识.


## 第二阶段: SFT(Supervised Fine-tuning)

有监督微调，构造指令微调数据集，在预训练模型基础上做指令精调，以对齐指令意图


## 第三阶段

(1)RLHF(Reinforcement Learning from Human Feedback)基于人类反馈对语言模型进行强化学习，分为两步：RM(Reward Model)奖励模型建模，构造人类偏好排序数据集，训练奖励模型，用来建模人类偏好，主要是"HHH"原则，具体是"helpful, honest, harmless";RL(Reinforcement Learning)强化学习，用奖励模型来训练SFT模型，生成模型使用奖励或惩罚来更新其策略，以便生成更高质量、更符合人类偏好的文. DPO(Direct Preference Optimization)直接偏好优化方法，DPO通过直接优化语言模型来实现对其行为的精确控制，而无需使用复杂的强化学习，也可以有效学习到人类偏好，DPO相较于RLHF更容易实现且易于训练，效果更好
